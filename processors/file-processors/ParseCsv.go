// package fileprocessors is generated by generate-processor tooling
// Make sure to insert real Description here
package fileprocessors


import (
    "bufio"
    "bytes"
    "context"
    "encoding/json"
    "errors"
    "github.com/percybolmer/workflow/failure"
    "github.com/percybolmer/workflow/metric"
    "github.com/percybolmer/workflow/processors/processmanager"
    "github.com/percybolmer/workflow/properties"
    "github.com/percybolmer/workflow/relationships"
    "strings"
)

var (
    //ErrNotCsv is triggered when the input file is not proper csv
    ErrNotCsv error = errors.New("this is not a proper csv file")
    //ErrHeaderMismatch is triggered when header is longer than CSV records
    ErrHeaderMismatch error = errors.New("the header is not the same size as the records")
)

// ParseCsv is a processor that is used to Read CSV in and
// Convert it into map[string]string and extract every CSV row into its own object
// Its generic and can accept any CSV
type ParseCsv struct{
    Name     string `json:"name" yaml:"name"`
    running  bool
    cancel   context.CancelFunc
    ingress  relationships.PayloadChannel
    egress   relationships.PayloadChannel
    failures relationships.FailurePipe
    *properties.PropertyMap `json:"properties,omitempty" yaml:"properties,omitempty"`
    *metric.Metrics `json:"metrics,omitempty" yaml:",inline,omitempty"`

    delimiter string
    headerlength int
    skiprows int
}

func init() {
    err := processmanager.RegisterProcessor("ParseCsv", NewParseCsvInterface)
    if err != nil {
        panic(err)
    }
}
// NewParseCsvInterface is used to register ReadFile
func NewParseCsvInterface() interface{}{
    return NewParseCsv()
}

// NewParseCsv is used to initialize and generate a new processor
func NewParseCsv() *ParseCsv {
    proc := &ParseCsv{
        egress: make(relationships.PayloadChannel, 1000),
        PropertyMap: properties.NewPropertyMap(),
        Metrics: metric.NewMetrics(),
        delimiter: ",",
        headerlength:  1,
        skiprows: 0,
    }

    proc.AddAvailableProperty("delimiter", "The character or string to use as a Delimiter")
    proc.AddAvailableProperty("headerlength", "How many rows the header is")
    proc.AddAvailableProperty("skiprows", "How many rows will be skipped in each file before starting to process")
    return proc
}

// Initialize will make sure all needed Properties and Metrics are generated
func (proc *ParseCsv) Initialize() error {

    // Make sure Properties are there
    ok, _ := proc.ValidateProperties()
    if !ok {
        return properties.ErrRequiredPropertiesNotFulfilled
    }

    delim := proc.GetProperty("delimiter")
    if delim != nil {
        proc.delimiter = delim.String()
    }
    headerlen := proc.GetProperty("headerlength")
    if headerlen != nil {
        newlen, err := headerlen.Int()
        if err != nil {
            return err
        }
        proc.headerlength = newlen
    }
    skiprow := proc.GetProperty("skiprows")
    if skiprow != nil {
        rows, err := skiprow.Int()
        if err != nil {
            return err
        }
        proc.skiprows = rows
    }
    // If you need to read data from Properties and add to your Processor struct, this is the place to do it
    return nil
}

// Start will spawn a goroutine that reads file and Exits either on Context.Done or When processing is finished
func (proc *ParseCsv) Start(ctx context.Context) error {
    if proc.running {
        return failure.ErrAlreadyRunning
    }
    // Uncomment if u need to Processor to require an Ingress relationship
    if proc.ingress == nil {
        return failure.ErrIngressRelationshipNeeded
    }

    proc.running = true
    // context will be used to spawn a Cancel func
    c, cancel := context.WithCancel(ctx)
    proc.cancel = cancel
    go func() {
        for {
            select {
                case payload := <-proc.ingress:
                    // This here is a tip to increase overall performance
                    // start ur processing inside a new goroutine, but make sure that i will cancel so
                    // we dont miss leaking goroutines
                    go func() {
                        data := payload.GetPayload()
                        rows, err := proc.Parse(data)
                        if err != nil {
                            proc.AddMetric("failures", "the number of failures that has occured in the processor", 1)
                            proc.failures <- failure.Failure{
                                Err:       err,
                                Payload:   payload,
                                Processor: "ParseCsv",
                            }
                        }
                        proc.AddMetric("inputfiles", "the number of input files that has occured", 1)
                        // Send each Row, or bulk send em?
                        for _, row := range rows {
                            proc.AddMetric("csvrows", "the number of csvrows outputted", 1)
                            proc.egress <- row
                        }
                    }()
                case <- c.Done():
                    return
            }
        }
    }()
    return nil
}

// Parse will take the payload expecting a byte array of a CSV file
// it will return a slice of all the rows found
func (proc *ParseCsv) Parse(payload []byte) ([]*CsvRow, error) {
    //reader := bytes.NewReader(payload)
    buf := bytes.NewBuffer(payload)
    scanner := bufio.NewScanner(buf)
    // index keeps track of line index
    var index int

    header := make([]string, 0)
    result := make([]*CsvRow, 0)
    for scanner.Scan() {
        line := scanner.Text()
        // Handle skip rows
        if index < proc.skiprows {
            index++
            continue
        }

        // Handle header rows
        values := strings.Split(line, proc.delimiter)
        if len(values) <= 1 {
            return nil, ErrNotCsv
        }
        // handle unique cases of multiline headers
        if index < (proc.skiprows + proc.headerlength) {
            header = append(header, values...)
            index++
            continue
        }
        // Make sure header is the same length as values
        if len(header) != len(values) {
            return nil, ErrHeaderMismatch
        }

        // Handle new rows
        newRow := &CsvRow{
            Payload: make(map[string]string, len(values)),
        }

        for i, value := range values {
            newRow.Payload[header[i]] = value
        }
        result = append(result, newRow)
    }

    return result, nil

}


// IsRunning will return true or false based on if the processor is currently running
func (proc *ParseCsv) IsRunning() bool {
    return proc.running
}
// GetMetrics will return a bunch of generated metrics, or nil if there isn't any
func (proc *ParseCsv) GetMetrics() []*metric.Metric {
    return proc.GetAllMetrics()
}
// SetFailureChannel will configure the failure channel of the Processor
func (proc *ParseCsv) SetFailureChannel(fp relationships.FailurePipe) {
    proc.failures = fp
}

// Stop will stop the processing
func (proc *ParseCsv) Stop() {
    if !proc.running {
        return
    }
    proc.running = false
    proc.cancel()
}
// SetIngress will change the ingress of the processor, Restart is needed before applied changes
func (proc *ParseCsv) SetIngress(i relationships.PayloadChannel) {
    proc.ingress = i
    return
}
// GetEgress will return an Egress that is used to output the processed items
func (proc *ParseCsv) GetEgress() relationships.PayloadChannel {
    return proc.egress
}


//CsvRow is a struct representing Csv data as a map
//Its also a part of the Payload interface
type CsvRow struct {
    Payload map[string]string `json:"payload"`
    Source  string            `json:"source"`
    Error   error             `json:"error"`
}

// GetPayloadLength will return the payload X Bytes
func (nf *CsvRow) GetPayloadLength() float64 {
    data, err := json.Marshal(nf.Payload)
    if err != nil {
        nf.Error = err
    }
    return float64(len(data))
}

// GetPayload is used to return an actual value for the Flow
func (nf *CsvRow) GetPayload() []byte {
    data, err := json.Marshal(nf.Payload)
    if err != nil {
        nf.Error = err
    }
    return data
}

//SetPayload will change the value of the Flow
func (nf *CsvRow) SetPayload(newpayload []byte) {
    nf.Error = json.Unmarshal(newpayload, &nf.Payload)
}

//GetSource will return the source of the flow
func (nf *CsvRow) GetSource() string {
    return nf.Source
}

//SetSource will change the value of the configured source
//The source value should represent something that makes it possible to traceback
//Errors, so for files etc its the filename.
func (nf *CsvRow) SetSource(s string) {
    nf.Source = s
}